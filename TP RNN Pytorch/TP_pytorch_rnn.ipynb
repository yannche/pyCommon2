{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reseaux de Neurones avec PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ce TP se situe dans la suite du TP precedent.\n",
    "#### Dans un premier temps, vous creerez ici un seul neurone avec pyTorch (equivalent a  la regression logistique)\n",
    "#### Ensuite, vous connecterez plusieurs neurones pour faire un reseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 0 - Quelques fonctions utiles (a ne pas lire. passez a la suite directement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_classifier(X, y, predict=None,ax=None, cmap='rainbow'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    #ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    if predict:\n",
    "        xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                             np.linspace(*ylim, num=200))\n",
    "        xxyy   = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z      = np.array([predict(Variable(t.from_numpy(d)).float()).data.numpy() for d in xxyy]).reshape(xx.shape)\n",
    "\n",
    "        # Create a color plot with the results\n",
    "        n_classes = len(np.unique(y.data.numpy()))\n",
    "        contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                               levels=np.arange(n_classes + 1) - 0.5,\n",
    "                               cmap=cmap, clim=(y.min(), y.max()),\n",
    "                               zorder=1)\n",
    "\n",
    "        ax.set(xlim=xlim, ylim=ylim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : construction manuelle d'un seul neurone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d'abord, je cree un mini jeu de donnees.\n",
    "\n",
    "X sera une matrice de n lignes et 2 colones, tiree au hazard\n",
    "\n",
    "pour chaque ligne i, y[i] sera une classe (0 ou 1) selon les coordonnees du point X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Variable(t.randn(n,2))\n",
    "y = X[:,0]-2*X[:,1]>0\n",
    "\n",
    "visualize_classifier(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, le code necessaire pour faire une regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Voici le code d'une regression logistique\n",
    "# C'est la meme chose qu'un neurone classique\n",
    "\n",
    "beta1 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "beta2 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "\n",
    "for trials in range(10*n):\n",
    "    # je choisis un exemple au hasard\n",
    "    i = np.random.randint(n)\n",
    "\n",
    "    # je calcule la prediction\n",
    "    z = beta1*X[i,0]+beta2*X[i,1]\n",
    "    a = 1/(1+t.exp(-z))\n",
    "\n",
    "    # je calcule le cout de cette prediction\n",
    "    if y.data[i] == 1:\n",
    "        e = -t.log(a)\n",
    "    else:\n",
    "        e = -t.log(1-a)\n",
    "\n",
    "    # je calcule les gradients\n",
    "    e.backward()\n",
    "\n",
    "    # je descend le gradient\n",
    "    beta1.data -= 0.1 * beta1.grad.data\n",
    "    beta2.data -= 0.1 * beta2.grad.data\n",
    "\n",
    "    # je remet le gradient a zero\n",
    "    beta1.grad.data.zero_()\n",
    "    beta2.grad.data.zero_()\n",
    "\n",
    "#print(\"predictions de probabilites = \",g(-(beta0+beta1*X)))\n",
    "print(beta1,beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice:\n",
    "Dans le code ci-dessus, la combinaison lineaire n'a pas d'intercept puisque `z=beta1*X[i,0]+beta2*X[i,1]`\n",
    "Rajoutez un intercept beta0 afin qu'on ait `z=beta0+beta1*X[i,0]+beta2*X[i,1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque: si on representait notre code sous la forme d'un neurone, ca donnerai ce dessin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](un_neurone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice:\n",
    "\n",
    "reorganisez le precedent code en ecrivant trois fonction `predict`,`cout` et `descend_gradient`. Pour cela, vous completerez le code qui suit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Voici le code d'une regression logistique\n",
    "# C'est la meme chose qu'un neurone classique\n",
    "\n",
    "beta0 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "beta1 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "beta2 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "\n",
    "def g(z):\n",
    "    return 1/(1+t.exp(-z))\n",
    "\n",
    "# fonction qui renvoie la prediction du neurone\n",
    "def predict(xi,beta0,beta1,beta2):\n",
    "    # ecrire ici le code manquant\n",
    "    ...\n",
    "\n",
    "# fonction qui calcule le cout\n",
    "def cout(yi,a):\n",
    "    # ecrire le code manquant\n",
    "    ...\n",
    "\n",
    "def descend_gradient(beta0,beta1,beta2):\n",
    "    # ecrire ici le code manquant\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for trials in range(10*n):\n",
    "    # je choisis un exemple au hasard\n",
    "    i = np.random.randint(n)\n",
    "\n",
    "    # je calcule la prediction\n",
    "    a = predict(X[i],beta0,beta1,beta2)\n",
    "\n",
    "    # je calcule le cout de cette prediction\n",
    "    e = cout(y.data[i],a)\n",
    "\n",
    "    # je calcule les gradients\n",
    "    e.backward()\n",
    "\n",
    "    # je descend le gradient et je le mets a zero\n",
    "    descend_gradient(beta0,beta1,beta2)\n",
    "\n",
    "# affichage graphique du resultat:\n",
    "visualize_classifier(X, y, partial(predict,beta0=beta0,beta1=beta1,beta2=beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Partie 2 : construction manuelle d'un petit reseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On va prendre des donnees non separables lineairement:\n",
    "X = Variable(t.randn(n,2))\n",
    "y = X[:,0]**2-2*X[:,1]>0\n",
    "\n",
    "visualize_classifier(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice:\n",
    "Implémentez le réseau ci-dessous, qui compte 12 parametres. Lancez l'apprentissage sur ces donnees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](reseau_de__neurones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si possible, essayez d'avoir une fonction globale `predict(xi)` qu'on pourra utiliser pour visualiser le classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice:\n",
    "Ecrivez une fonction qui calcule le taux d erreur de votre reseau sur votre jeu de donnees\n",
    "\n",
    "Essayez des architectures avec plus de couches. Qu'obtenez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
