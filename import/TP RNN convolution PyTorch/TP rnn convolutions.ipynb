{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch    as t\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Reseaux de neurones et Convolution\n",
    "\n",
    "Dans ce TP, vous utilisez le formalisme le plus haut niveau de pyTorch (très proche de Keras), qui masque tous les détails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 0 - fonction de visualisation (a ne pas lire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_pytorch_classifier(X, y, predict=None,**kwargs):\n",
    "    X_ = X.data.numpy()\n",
    "    y_ = y.data.numpy()\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X_[:, 0], X_[:, 1], c=y_, s=30, cmap='rainbow',\n",
    "               clim=(y_.min(), y_.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    #ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    if predict:\n",
    "        xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                             np.linspace(*ylim, num=200))\n",
    "        xxyy   = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z      = np.array([predict(Variable(t.from_numpy(d)).float(),**kwargs).data.numpy()\n",
    "                           for d in xxyy]).reshape(xx.shape)\n",
    "\n",
    "        # Create a color plot with the results\n",
    "        n_classes = len(np.unique(y.data.numpy()))\n",
    "        contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                               levels=np.arange(n_classes + 1) - 0.5,\n",
    "                               cmap='rainbow', #clim=(y_.min(), y_.max()),\n",
    "                               zorder=1)\n",
    "\n",
    "        ax.set(xlim=xlim, ylim=ylim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 - Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On reprend les memes donnees que dans le TP précédent\n",
    "# On va apprendre une régression logistique dessus\n",
    "n = 400\n",
    "\n",
    "# On crée des donnees non separables lineairement:\n",
    "X = Variable(t.randn(n,2))\n",
    "y = X[:,0]**2-2*X[:,1]>0\n",
    "y = y.float()\n",
    "\n",
    "visualize_pytorch_classifier(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),   # Couche lineaire z = w0+w1.x1+x2.x2.   Les parametres wj sont initialises aleatoirement\n",
    "    nn.Sigmoid()       # fonction de transfert sigmoide\n",
    ")\n",
    "\n",
    "fonction_de_cout = t.nn.BCELoss()  # fonction de cout de la regression logistique: y.log(p)+(1-y).log(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction sur l'exemple 0:\n",
    "y_pred = net(X[0])\n",
    "print( y_pred )\n",
    "\n",
    "# cout\n",
    "print( fonction_de_cout( y_pred,y[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tous les parametres sont stockes dans net.parameters\n",
    "print(list( net.parameters() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_learn(X,y,net,fonction_de_cout,ntrials=500,learning_rate = 0.05):\n",
    "\n",
    "    for trial in range(ntrials):\n",
    "        i = np.random.randint(0,n)\n",
    "        xi= X[i]\n",
    "\n",
    "        y_pred = net(xi)\n",
    "        cout   = fonction_de_cout(y_pred, y[i])\n",
    "\n",
    "        cout.backward()\n",
    "\n",
    "        for param in net.parameters():\n",
    "            param.data -= learning_rate * param.grad.data\n",
    "\n",
    "        net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_learn(X,y,net,fonction_de_cout,ntrials=500,learning_rate = 0.05)\n",
    "visualize_pytorch_classifier(X,y,net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 - Réseau de neurones multi-couches\n",
    "\n",
    "Ici, on va recoder avec `nn.Sequential` le réseau de neurones du TP précédent... mais en beaucoup moins de lignes de code !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée le meme réseau (a 2 couches cachées) que durant le dernier TP\n",
    "net2 = nn.Sequential(\n",
    "    nn.Linear(2, 3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(3,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# et on applique la meme procédure d'apprentissage\n",
    "simple_learn(X,y,net2,fonction_de_cout,ntrials=1000,learning_rate = 0.5)\n",
    "\n",
    "# et on visualise\n",
    "visualize_pytorch_classifier(X,y,net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Affichez les poids des différents neurones\n",
    "#### Q: Construisez un réseau à 3 couches cachées et deux neurones par couche cachée, et entrainez-le de nouveau\n",
    "#### Q: Créez une base de test `X_test` et `y_test` en utilisant la même procédure de création de données que pour `X` et `y`, et mesurez l'erreur de classification comise par ces réseaux sur ce jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 - Réseau de neurones multi-couches appliqué aux données MNIST\n",
    "\n",
    "Le code ci-dessous (il n'est pas utile de le lire) charge 2000 images de la base d'images de chiffres MNIST.\n",
    "\n",
    "Les 1000 premieres sont stockees dans `X[i]`, et les 1000 suivantes dans `X_test[i]` sous forme de tableau `28x28`.\n",
    "\n",
    "De meme, `y[i]` et `y_test[i]` identifient le chiffre correspondant à l'images `X[i]` et   `X_test[i]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "mnist = datasets.MNIST(root='./data/',\n",
    "                       train=True,\n",
    "                       transform=transforms.ToTensor(),\n",
    "                       download=True)\n",
    "\n",
    "mnist_data_loader = t.utils.data.DataLoader(dataset=mnist,\n",
    "                                            batch_size=1000, \n",
    "                                            shuffle=True)\n",
    "\n",
    "mnist_data = next(iter(mnist_data_loader))\n",
    "X,y = Variable(mnist_data[0].squeeze()),Variable(mnist_data[1])\n",
    "\n",
    "mnist_data = next(iter(mnist_data_loader))\n",
    "X_test,y_test = Variable(mnist_data[0].squeeze()),Variable(mnist_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classe = \",y.data[0])\n",
    "plt.imshow(X.data[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q les images dans `X` sont des matrices 28x28. En entrée du réseau, vous devrez avoir ici des vecteurs et non des matrices, pour chaque exemple. Il faut donc \"applatir\" ces matrices en vecteurs. En numpy, cela se fait avec `reshape` et en pytorch avec `view`, qui s'utilise à peu près comme `reshape`. Appliquez `view` sur `X` pour que `X` devienne une matrice ou chaque ligne représente une image de 784 pixels (c'est à dire 28*28)\n",
    "\n",
    "\n",
    "#### Q: Telle qu'on l'a vu jusqu'à présent, la régression logistique permet de traiter des problèmes d'apprentissage à deux classes.  On cherchera donc à prédire si l'image d'un nombre représente un chiffre supérieur ou égal à 5, ou non. Reformulez `y` de telle sorte que `y[i]` soit égal à 1 si le chiffre est supérieu ou égal à 5, et 0 sinon\n",
    "\n",
    "#### Q: Appliquez la régression logistique du début du TP sur cette tâche, sur les images X.\n",
    "\n",
    "#### Q: Calculez le taux d'erreur du modèle obtenu sur X_test\n",
    "\n",
    "#### Q: recommencez avec un réseau de neurones plus complexe que précédemment.\n",
    "\n",
    "#### Q: Insérez une couche de \"dropout\" dans le modèle (http://pytorch.org/docs/master/nn.html#dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3 - Réseau de Convolution appliqués à MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: On n'applique pas des convolutions sur des vecteurs, mais sur des tenseurs. Transormez `X` pour que sa nouvelle dimension soit 1000x1x28x28  (c'est à dire 1000 images, 1 canal, largeur 28, hauteur 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant créer un réseau de convolution.\n",
    "\n",
    "#### Q: construisez un réseau comportant une couche de convolution 3x3 avec un seul canal, suivie d'une fonction d'activation ReLU. Les fonctions à utiliser sont `nn.Conv2d(in_channels, out_channels, kernel_size)` et `nn.ReLU`.\n",
    "\n",
    "#### Q: Si vous appliquez votre réseau sur une image, qu'obtenez-vous ? Affichez le résultat comme une image. Faites bien attention aux dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q: maintenant, écrivez deux modèles Sequential: un réseau qui prend un tenseur d'image en entrée, et qui fait une couche de convolutions et renvoie un tenseur d'image en sortie. Un autre qui crée un applique une régression logistique sur un vecteur de taille 784. Ecrivez une fonction `predict` qui calcule le résultat de l'application successive de ces deux réseaux sur un tenseur image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
