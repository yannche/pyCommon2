{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression avec PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch est un outil similaire a numpy. Il permet de faire des calculs sur des tableaux 1D,2D,3D,4D (ou plus). En pytorch, on appelle ces tableaux des tenseurs.\n",
    "\n",
    "#### La principale difference entre les tenseurs pyTorch et les tableaux  numpy est que les tenseurs pyTorch peuvent etre envoyes sur une carte graphique pour etre traites plus rapidement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : les tenseurs en pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on cree:\n",
    "#     un vecteur de taille 2,\n",
    "x1 = t.Tensor([10,20])\n",
    "\n",
    "#     une matrix 2x1,\n",
    "x2 = t.Tensor([[10],[20]])\n",
    "\n",
    "#     une matrice 1x2,\n",
    "x3 = t.Tensor([[10,20]])\n",
    "\n",
    "#     une matrce 3x4 remplie de zeros\n",
    "x4 = t.zeros(3,4)\n",
    "\n",
    "#     une matrice aleatoire (loi normale) 2x2\n",
    "x5 = t.randn(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez dans la cellule ci-dessous les differents tableaux crees ci-dessus, pour bien comprendre ces commandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comme avec numpy, on peut faire des tas d'operations sur ces tableaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# j'ajoute 3 a toutes les cases du tableau x1\n",
    "x5 = x1 + 3\n",
    "# j'eleve au carre toutes les cases du tableau x4\n",
    "x6 = x5 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez dans la cellule ci-dessous les differents tableaux crees ci-dessus, pour bien comprendre ces commandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut convertir des donnees entre numpy et pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conversion d'un tableau numpy vers pytorch\n",
    "z1  = np.zeros( (2,2) )\n",
    "x6 = t.from_numpy( z1 )\n",
    "\n",
    "# conversion d'un tableau pytorch vers numpy\n",
    "z2 = x6.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez dans la cellule ci-dessous les differents tableaux crees ci-dessus, pour bien comprendre ces commandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations utiles de pyTorch, similaires aux operations disponibles dans numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transposition\n",
    "x7 = x2.t()\n",
    "\n",
    "# changement de forme (comme reshape en numpy)\n",
    "# ici, on convertit une matrice 1x2 en matice 2x1\n",
    "x8 = x7.view(2,1)\n",
    "\n",
    "\n",
    "# remplir de zero un tableau existant\n",
    "x6.zero_()\n",
    "\n",
    "# multiplication de matrice\n",
    "x9 = t.mm( x8 , x7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez dans la cellule ci-dessous les differents tableaux crees ci-dessus, pour bien comprendre ces commandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# acceder aux lignes, colonnes, et elements d'un tenseur, c'est comme avec numpy\n",
    "print(x9[0,:],x9[:,0],x9[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exercice:\n",
    "Creez un tenseur 10x10 rempli avec des uns,\n",
    "mettez des deux sur la diagonale, et elevez-le au carre, et affichez le resultat comme un tableau numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  4.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  4.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  4.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  4.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  4.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  4.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  4.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  4.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  4.]]\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(10,10)+1\n",
    "x2= x + t.diag(t.ones(10))\n",
    "x3= x2**2\n",
    "print(x3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : les variables en pyTorch\n",
    "\n",
    "(cette partie est tres succincte. pour d'autres infos, voir ici: http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html )\n",
    "\n",
    "En pytorch, une variable est une boite qui contient un tenseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = t.Tensor([[10,20]])\n",
    "v1 = Variable( x1 )  # on aurait pu ecrire directement v1=Variable(t.Tensor([10,20]))\n",
    "\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on peut recupere le tenseur a l'interieur de la boite de cette facon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = v1.data\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les operations sur les variables pyTorch sont les *memes* que celles sur les tenseurs (addition, multiplication, transposition, conversion, etc..)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2 = (v1 ** 2).t()\n",
    "print( v2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut faire ces memes operations sur les tenseurs associes aux variables, mais ces operations ne seront pas enregistrees dans le graphe de computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t10 = (v1.data ** 2).t()\n",
    "print(t10)    # t10 est un tenseur, pas une variable, donc on ne peut pas calculer une derivee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'interet des variables pyTorch est que toutes les operations faites sur ces variables sont enregistr√©es (sous la forme d'un graphe de computation, mais c'est transparent pour vous).\n",
    "\n",
    "Donc pytorch peut calculer une derivee ou un gradient automatiquement si vous le souhaitez a partir de ce graphe.\n",
    "\n",
    "Un exemple de ce type de graphe est visible ici: https://cdn-images-1.medium.com/max/1600/1*5PLIVNA5fIqEC8-kZ260KQ.gif\n",
    "\n",
    "Pour indiquer qu'on voudra plus tard la derivee par rapport a une variable donnee, cette variable doit etre cree avec l'option *requires_grad=True*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple, calculons (v-20)^2, pour v=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = Variable( t.Tensor([10]) , requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2 = (v1 - 20) ** 2\n",
    "print('valeur de v2=',v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, demandons a pyTorch de deriver v2, comme si v2 etait une fonction de v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ici, je demander de calculer la derivee de v2 par rapport a toutes les variables\n",
    "v2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La derivee de v2 par rapport a v1 est disponible dans v1.grad.\n",
    "* la commande backward va calculer le gradient et l'ajouter a v1.grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(v1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que v1.grad est elle-meme une variable. On peut donc acceder au tenseur associ√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( v1.grad.data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si je refais exactement les memes operations, le nouveau gradient, identique a l'ancien, est rajoute a v1.grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2 = (v1 - 20) ** 2\n",
    "v2.backward()\n",
    "print(v1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on calcule le gradient plusieurs fois, il faut donc remettre a zero le tenseur du gradient a chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1.grad.data.zero_()\n",
    "v2 = (v1 - 20) ** 2\n",
    "v2.backward()\n",
    "print(v1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - la descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, lorsque vous executez backward, le graphe de computation est efface (sauf les variables que vous avez crees). Donc pour relancer backward, il faut relancer les calculs aussi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# descente de gradient\n",
    "v1 = Variable( t.Tensor([10]) , requires_grad=True)\n",
    "for i in range(500):\n",
    "    v2 = (v1 - 20) ** 2\n",
    "    v2.backward()\n",
    "    v1.data -= 0.01*v1.grad.data   # on ne veut pas enregistrer ca dans le calcul du gradient\n",
    "    v1.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut faire la meme chose en mettant le calcul de v2 dans une fonction et en mettant un pas de descente de gradient dans une fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mafonction(v):\n",
    "    return (v - 20) ** 2\n",
    "\n",
    "def descendGradient(v):\n",
    "    v1.data -= 0.01*v1.grad.data   # on ne veut pas enregistrer ca dans le calcul du gradient\n",
    "    v1.grad.data.zero_()\n",
    "\n",
    "v1 = Variable( t.Tensor([10]) , requires_grad=True)\n",
    "\n",
    "for i in range(500):\n",
    "    v2 = mafonction(v1)\n",
    "    \n",
    "    v2.backward()\n",
    "    \n",
    "    descendGradient(v1)\n",
    "\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "\n",
    "On veut coder une descente de gradient sur fonction:\n",
    "\n",
    "f(theta) = sum (theta-xi)^2\n",
    "avec x=(10,20,30,40,50,60,70,80,90)\n",
    "\n",
    "autrement dit, on a f(theta)=(theta-10)^2 + (theta-20)^2 + (theta-30)^2 + ...\n",
    "\n",
    "Ecrivez cela en pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Je propose deux corrections possibles a cet exercice\n",
    "##### correction 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 50.0000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(t.Tensor([10,20,30,40,50,60,70,80,90]))\n",
    "theta = Variable(t.Tensor([0]),requires_grad=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    f = t.sum((theta-x)**2)\n",
    "    f.backward()\n",
    "    theta.data -= 0.1 * theta.grad.data\n",
    "    theta.grad.data.zero_()\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### correction 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 50.0000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(t.Tensor([10,20,30,40,50,60,70,80,90]))\n",
    "theta = Variable(t.Tensor([0]),requires_grad=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    f = Variable(t.Tensor([0]))\n",
    "    \n",
    "    for i in range(9):\n",
    "        f = f + (theta-x[i])**2\n",
    "    \n",
    "    f.backward()\n",
    "    theta.data -= 0.1 * theta.grad.data\n",
    "    theta.grad.data.zero_()\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2 (regression lineaire)\n",
    "\n",
    "Soit les donnees suivantes, generees aleatoirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11476c588>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJBDhDgAJRLgDQAIR7gCQQIQ7ACTQ/wOhO6iuuxaXHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d3db518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = t.randn(100)\n",
    "y = t.sin(x/2+1)+4\n",
    "plt.scatter(x.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut faire une regression lineaire simple sur ces donnees. Pour rappel, en regression lineaire simple, on defini deux parametres beta0 et beta1, et pour un valeur xi donnee, la prediction est obtenue en faisant zi = beta0+beta1.xi\n",
    "De meme l'erreur totale de prediction est la somme (yi-zi)^2 pour tous les i.\n",
    "\n",
    "Vous ferez ceci:\n",
    "* On va definir deux parametres pour la regression lin√©aire: beta0 et beta1\n",
    "* definissez Z, la prediction donnee par la regression lineaire pour chaque point.\n",
    "* definissez E, l'erreur de prediction\n",
    "* appliquez l'algorithme de descente de gradient pour minimiser E, et affichez le resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x115571e48>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncBcRaUEKdxGRFqRwFxFpQQp3EZEW9P8BOaivCU/MT40AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115499630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Correction\n",
    "\n",
    "xx = Variable(x)\n",
    "yy = Variable(y)\n",
    "\n",
    "beta0 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "beta1 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    z = beta0+beta1*xx\n",
    "    e = t.sum((yy-z)**2)\n",
    "    \n",
    "    e.backward()\n",
    "\n",
    "    beta0.data -= 0.001 * beta0.grad.data\n",
    "    beta1.data -= 0.001 * beta1.grad.data\n",
    "\n",
    "    beta0.grad.data.zero_()\n",
    "    beta1.grad.data.zero_()\n",
    "\n",
    "plt.scatter(x.numpy(),y.numpy())\n",
    "plt.scatter(x.numpy(),z.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 3 (regression logistique)\n",
    "\n",
    "Implementez la regression logistique sur l'exemple vu en TD (le tableau de 6 donnees + la donnee supplementaire). Lancez la descente de gradient dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions de probabilites =  Variable containing:\n",
      " 0.5149\n",
      " 0.5149\n",
      " 0.7462\n",
      " 0.7462\n",
      " 0.7462\n",
      " 0.7462\n",
      " 0.3389\n",
      "[torch.FloatTensor of size 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correction\n",
    "\n",
    "X = Variable(t.Tensor([5,5,12,12,12,12,0]))\n",
    "y = Variable(t.Tensor([1,0,1,1,1,0,0]))\n",
    "\n",
    "beta0 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "beta1 = Variable(t.Tensor([0]),requires_grad=True)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    z = beta0+beta1*X\n",
    "    g = 1/(1+t.exp(-z))\n",
    "\n",
    "    e = -y*t.log(g)  - (1-y)*t.log(1-g)\n",
    "\n",
    "    e = t.mean(e)\n",
    "    \n",
    "    e.backward()\n",
    "\n",
    "    beta0.data -= 0.1 * beta0.grad.data\n",
    "    beta1.data -= 0.1 * beta1.grad.data\n",
    "\n",
    "    beta0.grad.data.zero_()\n",
    "    beta1.grad.data.zero_()\n",
    "\n",
    "print(\"predictions de probabilites = \",1/(1+t.exp(-(beta0+beta1*X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 4 (regression lineaire multiple)\n",
    "\n",
    "Implementez la regression lineaire avec pyTorch sur les donnees de diabetes chargees ci-dessous. Ce dataset comporte 10 attributs decrivant le patient (age, sex, weight, blood pressure) et 442 patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correction\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "X,y = datasets.load_diabetes(True)\n",
    "X   = np.hstack((np.ones((442,1)),X))\n",
    "\n",
    "n,d = X.shape\n",
    "\n",
    "X = Variable(t.Tensor(X))\n",
    "y = Variable(t.Tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta  = Variable(t.randn(d,1),requires_grad=True)\n",
    "\n",
    "for epoch in range(30):\n",
    "    z = t.mm(X,beta)\n",
    "    e = t.mean( (y-z)**2 )\n",
    "    \n",
    "    e.backward()\n",
    "\n",
    "    beta.data -= 0.1 * beta.grad.data\n",
    "    beta.grad.data.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
